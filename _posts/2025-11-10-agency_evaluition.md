---
layout: post
title: "High Intelligence之外，模型和人都需要High Agency"
date: 2025-11-10
description:  "Agency Evluation"
tags: [life, entrepreneur, cn]
lang: cn
---
在参加各种talk的时候，主持人非常喜欢问一个问题就是公司想要招收什么样的人，一般回答就是High Agency，这个词也有在PH.D.招生领域叫做self-motivated,在字节叫没活找活的ownership。那么同样的这种能力对于AI也同样重要，所以我们究竟怎么定义和理解Agency

当下的AI领域，“Agency”（或“智能体”）无疑是最火热的词汇。但当我们谈论它时，我们到底在谈论什么？

许多人将其等同于**工具调用**、**指令遵循**、**任务规划**。但这其实是对“Agency”的极大误解。

在深入探讨之前，让我们先勾勒一个**“高智商，但毫无Agency”**的大语言模型（LLM）形象。这个模型可能：
- 拥有海量的**世界知识**；
- 具备强大的**长文本处理**能力；
- 在**推理**和**代码**任务上得分极高。
    
然而，当你交给它一个任务时，它的表现会是这样的：

1. **盲目执行，缺乏意图理解：** 它能出色地遵循你的每一个指令，但从不思考任务背后的**“为什么”**。你给的指示哪怕是荒谬或低效的，它也会一丝不苟地执行。就像一个从不思考“这项工作是否合理”的员工，它缺乏全局观，无法通过理解真实意图来优化任务。
    
2. **浅尝辄止，极易放弃：** 它会尝试你给定的（或它自己规划的）两三种方法。一旦工具调用出错或遇到障碍，它会立即停止，然后告诉你一个错误结果，或者干脆放弃，而不是去探索“为什么会失败”和“还有没有别的路”。
    
3. **自信“乱答”，缺乏自知之明：** 它总能交付一个“结果”，但对这个结果的质量、是否真正完成了任务，它毫无感知。它无法自我评估（Self-Evaluate），导致它每次都可能自信满满地给出一个错误或无效的答案。
    

真正的Agency，不是上述任何一种。它是一个模型在特定环境中，根据人类的真实需求和可用的工具，**自主找到一条最优路径来完成任务的能力**。

要衡量这种高级能力，我们不能再用传统的Benchmark。我们需要一套全新的评估维度。

---

### 一、元思考能力

我们首先要破除一个迷思：在Agent任务上，**“指令遵循”可能是一个陷阱**。用户未必总能给出最高效、最合理的执行思路。

低Agency的模型会忠实地“服从”一个错误的指令；而高Agency的模型则会“反思”这个指令。

- **元思考（Meta-Cognition）**：指模型超越指令本身，去思考任务的**根本目的**和**合理性**，并基于这个目的去寻找最优解，哪怕这需要它“质疑”用户的提议。
    

> **场景对比：**
> 
> - **场景1：无意义的任务。** 当用户要求模型在一个巨大的数据库中寻找一个被刻意埋藏的、毫无意义的字符串（如“大海捞针”式的Benchmark）。
>     
>     - **低Agency模型：** 启动任务，开始搜索，直到超时或失败。
>         
>     - **高Agency模型：** 反问用户：“您似乎在进行一项测试。请问这个任务的真实目的是什么？它看起来没有实际意义。”
>         
> - **场景2：低效的技术选型。** 用户要求模型使用Selenium（一种UI自动化工具）去爬取一个静态网站的数据。
>     
>     - **低Agency模型：** 启动浏览器，加载页面，模拟点击，缓慢抓取。
>         
>     - **高Agency模型：** 提议：“我分析了目标网站，它是一个静态页面。使用Selenium过于笨重且低效。我建议使用更轻量的`requests` + `BeautifulSoup`库，速度可以提升10倍。您同意吗？”
>         

**如何衡量：**

- **错误指令纠错率：** 当给定一个明显低效或错误的技术路径时，模型能否识别并提出更优方案？
    
- **路径高效率：** 面对一个开放任务，模型自己选择的路径（`L_model`）与已知的最优路径（`L_optimal`）相比，效率有多高？（即 `L_optimal` / `L_model`）
    

---

### 二、探索能力

现实世界的任务充满了不确定性。一个足够难的、链路足够长的任务，必然会遇到各种失败和死胡同。模型的探索能力，就是它绕过障碍、寻找新路径的能力。

- **探索力（Exploration）**：指模型在面对失败和错误时，能否**主动分析失败原因**，并**探索出足够多的、全新的解决方案**来绕过问题，而不是在同一个地方反复重试。
    

> **场景对比：**
> 
> - **场景1：环境阻碍。** 模型在尝试访问GitHub时（例如在中国大陆环境）遭遇网络连接失败。
>     
>     - **低Agency模型：** 反复重试 `git clone`，最后报告“网络错误”。
>         
>     - **高Agency模型：** 识别到网络问题后，会主动尝试搜索GitHub的镜像网站并使用镜像URL；甚至可能建议配置代理（如VPN或端口映射）来解决根本的环境问题。
>         
> - **场景2：版本不兼容。** 模型写的代码跑起来了，但结果“对，但不完全对”（例如因依赖库版本不兼容导致了细微的计算偏差）。
>     
>     - **低Agency模型：** 假定是代码逻辑出了漏洞，开始在主逻辑上“打补丁”，试图修正结果，使代码越来越复杂。
>         
>     - **高Agency模型：** 发现结果偏差后，会怀疑是环境问题。主动检查依赖库的版本，发现版本过旧，并尝试升级版本来解决问题。
>         

**如何衡量：**

- **错误恢复能力：** 面对工具调用失败（如API key失效、网络不通、文件找不到），模型是直接放弃，还是能主动设计出绕过（Workaround）的方案？
    
- **错误重复率：** 模型是否会在同一个问题上（如同一类错误信息、或70%相似的错误日志）反复“摔倒”？高Agency模型应该能识别并规避重复错误。
    

---

### 三、任务完成意识

一个模型是否“知道”自己是否完成了任务？它对自己的“产出”是否有质量评估？

在执行科研或长期产品开发这类高难度任务时，人类会给自己设定**里程碑（Milestones）**和**假设（Hypothesis）**，以确保自己“work on the right track”。高Agency的模型也应如此。

- **任务完成意识（Task Completion Awareness）**：指模型能够**自我评估**当前的任务进度和产出质量，并为复杂任务**主动设计合理的关键节点（Milestone）**和**量化指标**。
    

> **场景对比：**
> 
> - **场景1：规划旅行。** 用户说：“帮我规划一个7天的日本行程。”
>     
>     - **低Agency模型：** 随机生成一个报告，用刻板印象（东京、大阪、寿司、富士山）堆满行程，景点之间可能相距甚远，完全不考虑交通可行性。然后自信地交付。
>         
>     - **高Agency模型：** 生成初版行程后，会**自我评估**：“这些地点之间的交通是否合理？餐厅评分是否真实？换酒店是否太频繁？” 它会使用地图工具计算距离，查询餐厅评价，为自己的规划“打分”。
>         
> - **场景2：编写代码。** 用户说：“我需要一个‘小鸟起飞’的代码。”
>     
>     - **低Agency模型：** 生成一段Pygame代码，然后直接交付给用户。
>         
>     - **高Agency模型：** 生成代码后，会**主动尝试在虚拟环境中运行**。如果运行失败（如缺少库、有bug），它会继续调试，直到代码能真正跑通，再交付给用户。
>         

**如何衡量：**

- **关键节点自设计：** 在一个需要10步以上才能完成的复杂任务中，模型能否主动设计出3-5个合理的中间检查点（Milestones）？
    
- **过程评估频率：** 在整个任务流程中，模型调用“自我评估”或“质量检查”功能的次数。
    

---

### 四、模型合作能力

再强大的模型也有其局限性。高Agency的体现，是**清晰地认识到自己的边界**，并知道在何时、如何“求助”。

这种求助可以是引入其他模型（专家Agent），也可以是调动人类（Human in the loop），或者是将任务拆解后并行分发。

- **合作能力（Collaboration）**：指模型能够**识别出任务的复杂性和自己的能力边界**，主动将任务拆解，并将自己无法高效处理的部分**分发**给其他更适合的Agent（或人类）来协同完成。
    

> **场景对比：**
> 
> - **场景1：复杂数据分析。** 用户要求分析一个包含多个维度的大型数据集。
>     
>     - **低Agency模型：** 试图自己一步步完成所有工作：清洗、统计、可视化，过程漫长且容易出错。
>         
>     - **高Agency模型：** **识别出可并行的子任务**。它会说：“我可以将任务分解：同时调用‘数据清洗Agent’处理缺失值，调用‘统计分析Agent’计算相关性。我将负责整合它们的报告。”
>         
> - **场景2：跨领域产品设计。** 用户要求设计一个医疗AI产品。
>     
>     - **低Agency模型：** 开始“一本正经地胡说八道”，编造医学术语和产品功能。
>         
>     - **高Agency模型：** 立即**识别到能力边界**：“这是一个严肃的医疗任务。我将为您调用‘医学专家Agent’来确保诊断逻辑的准确性，并调用‘法律合规Agent’来检查它是否符合HIPAA（医疗隐私）规定。”
>         

**如何衡量：**

- **并行任务发现率：** 在可被拆解的任务中，模型能否自动识别并设计出并行执行的路径？
    
- **协作增益（Difference）：** 衡量“模型自己解决”和“模型通过involve其他Agent解决”之间的差异。差异越大（例如完成时间缩短80%、准确率提升50%），证明其协作决策越有效。

### 结语

Agency，绝不是简单的自动化执行。

它是一种包含**元思考**、**强探索性**、**自我意识**和**协作智能**的复合能力。